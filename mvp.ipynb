{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.base import clone\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROLS\n",
    "\n",
    "#model = LogisticRegression(max_iter=10000)\n",
    "#model = RandomForestClassifier()\n",
    "model = xgb.XGBClassifier(eval_metric='error', use_label_encoder=False)\n",
    "\n",
    "#missingness_threshold = 0.9\n",
    "#missingness_threshold = 0.75\n",
    "#missingness_threshold = 0.5\n",
    "missingness_threshold = 0.25\n",
    "\n",
    "#outcome = 'alc_postlt'\n",
    "outcome = 'harmfuldrink'\n",
    "\n",
    "#imputation_strategy = '-1'\n",
    "#imputation_strategy = 'mode'\n",
    "imputation_strategy = None # only for XGBoost\n",
    "\n",
    "patient_questions_only = True\n",
    "#patient_questions_only = False\n",
    "\n",
    "include_feature_selection = True\n",
    "#include_feature_selection = False\n",
    "\n",
    "validation_center = 1 # choose from: 1-12 inclusive\n",
    "\n",
    "save_results = True\n",
    "#save_results = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata('./data/psychosocial_data.dta') # read in questions\n",
    "print(df.shape)\n",
    "df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_data = df.filter(regex='^q[0-9]{1,3}$',axis=1) # filter to columns containing the 199 questions asked\n",
    "question_data = pd.concat([question_data, df[['id', 'nid']]], axis=1) # keep id and nid columns\n",
    "question_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_patient_questions = ['q1','q2','q3','q4','q5','q6','q7','q8','q9','q10','q12','q22',\n",
    "                         'q33','q71','q129','q130','q145','q198']\n",
    "mixed_questions = ['q28','q70','q146'] # 0 and NaN mean missing value\n",
    "special_mixed_questions = ['q27','q40'] # only NaN means missing value\n",
    "\n",
    "if patient_questions_only:\n",
    "    question_data.drop(columns=non_patient_questions+mixed_questions+special_mixed_questions, inplace=True)\n",
    "else: # split mixed questions\n",
    "    def mixed_question_splitter(row, q): # return 1 if question has non-zero and non-null value\n",
    "        if np.isnan(row[q]) or row[q] == 0:\n",
    "            return 0\n",
    "        return 1\n",
    "    \n",
    "    for q in mixed_questions: \n",
    "        new_label = q + '_split'\n",
    "        question_data[new_label] = df.apply(lambda row: mixed_question_splitter(row, q), axis=1)\n",
    "        \n",
    "    def special_mixed_question_splitter(row, q): # return 1 if question has only non-null value\n",
    "        if np.isnan(row[q]):\n",
    "            return 0\n",
    "        return 1\n",
    "        \n",
    "    for q in special_mixed_questions: \n",
    "        new_label = q + '_split'\n",
    "        question_data[new_label] = df.apply(lambda row: special_mixed_question_splitter(row, q), axis=1)\n",
    "    \n",
    "question_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize missingness\n",
    "\n",
    "question_data_unlabeled = question_data.drop(columns=['id', 'nid'])\n",
    "q_nullseries = question_data_unlabeled.isna().sum() / question_data_unlabeled.shape[0] # calculate % NaN per column\n",
    "ax = q_nullseries.plot.bar(x='question', y='% missing', rot=50, figsize=(50,20)) # plot question vs. % missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse rows with the same ID by picking the one with the least missingness\n",
    "\n",
    "for i in question_data.id.unique(): # for each unique id...\n",
    "    temp = question_data[question_data.id == i].copy() # extract rows with corresponding id\n",
    "    if temp.shape[0] > 1: # if there is more than one entry...\n",
    "        idx = temp.isna().sum(axis=1).idxmin() # get the id of the row with min missing\n",
    "        idxs = list(temp.index.values)\n",
    "        idxs.remove(idx)\n",
    "        question_data.drop(index=idxs, inplace=True)\n",
    "        \n",
    "question_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with a higher % missing than our threshold\n",
    "\n",
    "cols_to_drop = []\n",
    "    \n",
    "for ind,val in q_nullseries.iteritems():\n",
    "    if val > missingness_threshold:\n",
    "        cols_to_drop.append(ind)\n",
    "\n",
    "question_data.drop(columns=cols_to_drop, inplace=True)\n",
    "question_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outcomes = pd.read_stata('./data/full_cohort_with_clinical_outcomes.dta') # older version of outcome data\n",
    "outcomes = pd.read_stata('./data/test.dta') # read outcome data\n",
    "print(outcomes.shape)\n",
    "outcomes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = outcomes[[outcome, 'ptid', 'center', 'hospdeath']] # extract outcome, location, and ID data\n",
    "outcomes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = question_data.merge(outcomes, left_on='id', right_on='ptid', how='left') # merge with question data\n",
    "print(combined.shape)\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined[combined.hospdeath != 1] # remove patients who died in the hospital\n",
    "combined.drop(columns=['hospdeath'], inplace=True)\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[outcome].value_counts() / combined.shape[0] # outcome distribution (percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined[outcome].value_counts() # outcome distribution (raw counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[outcome].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined[outcome].fillna(0, inplace=True) # replace missing outcome with 0 meaning no harmful drinking\n",
    "combined.drop(inplace=True, columns=['id', 'ptid', 'nid']) # remove merging variables\n",
    "print(combined.shape)\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv('./data/data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserve one center for validation\n",
    "\n",
    "val = combined[combined.center == validation_center] # extract center for validation\n",
    "val_X = val.drop(columns=[outcome]) # extract predictors\n",
    "val_y = val[outcome] # extract outcome\n",
    "print(val_X.shape, val_y.shape)\n",
    "\n",
    "combined = combined[combined.center != validation_center] # remove center from rest of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined.drop(columns=[outcome]) # extract predictors\n",
    "y = combined[outcome] # extract outcome\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing data\n",
    "\n",
    "if imputation_strategy == '-1':\n",
    "    X.fillna(-1, inplace=True) # replace NaN with -1 to signify ommitted question\n",
    "elif imputation_strategy == 'mode':\n",
    "    for col in X.columns:\n",
    "        X[col].fillna(X[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_feature_selection(model, X, y):\n",
    "    features = list(X.columns)\n",
    "    selected_features = []\n",
    "    scores = []\n",
    "    \n",
    "    for i in tqdm(range(X.shape[1])):\n",
    "        best_score = 0\n",
    "        next_feat = ''\n",
    "        for feat in features:\n",
    "            selected_features.append(feat)\n",
    "            temp_X = X[selected_features]\n",
    "            temp_scores = cross_validate(clone(model), temp_X, y, cv=5, scoring=['recall'])\n",
    "            temp_score = temp_scores['test_recall'].mean()\n",
    "            if temp_score >= best_score:\n",
    "                best_score = temp_score\n",
    "                next_feat = feat\n",
    "            selected_features.pop()\n",
    "        #print('Added Feature:', next_feat)\n",
    "        selected_features.append(next_feat)\n",
    "        features.remove(next_feat)\n",
    "        scores.append(best_score)\n",
    "        \n",
    "    #print('Ordering of Features:', selected_features)\n",
    "    \n",
    "    plt.title('Forward Feature Selection')\n",
    "    plt.xlabel('# of Features')\n",
    "    plt.ylabel('recall')\n",
    "    plt.plot(list(range(X.shape[1])), scores)\n",
    "    return scores, selected_features\n",
    "\n",
    "if include_feature_selection:\n",
    "    scores, selected_features = forward_feature_selection(model, X, y)\n",
    "    optimal_n_features = scores.index(max(scores)) + 1\n",
    "    print('\\nOptimal number of features:', optimal_n_features)\n",
    "    optimal_features = selected_features[:optimal_n_features]\n",
    "    X = X[optimal_features]\n",
    "    val_X = val_X[optimal_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(clone(model), X, y, cv=5, scoring=['accuracy', 'precision', 'recall', 'roc_auc'])\n",
    "\n",
    "print('Accuracy:', round(scores['test_accuracy'].mean(), 4), \"+/-\", round(scores['test_accuracy'].std()*2, 4))\n",
    "print('Precision/PPV:', round(scores['test_precision'].mean(), 4), \"+/-\", round(scores['test_precision'].std()*2, 4))\n",
    "print('Recall:', round(scores['test_recall'].mean(), 4), \"+/-\", round(scores['test_recall'].std()*2, 4))\n",
    "print('AUC:', round(scores['test_roc_auc'].mean(), 4), \"+/-\", round(scores['test_roc_auc'].std()*2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation matrix of selected features\n",
    "\n",
    "corrMatrix = X.corr()\n",
    "sn.heatmap(corrMatrix, annot=True)\n",
    "plt.rcParams['figure.figsize'] = 15, 15\n",
    "plt.title('Correlation Matrix: Question Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in X.columns:\n",
    "    print('--' + feat + '--')\n",
    "    try:\n",
    "        print('NaN ', q_nullseries[feat])\n",
    "    except KeyError: # center variable not in q_nullseries, default to 0 missing values\n",
    "        print('NaN  0')\n",
    "    print(X[feat].value_counts(normalize=True)) # get value counts for our selected features for qualitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have model predict on validation center to gague performance\n",
    "\n",
    "val_y_pred = model.predict(val_X)\n",
    "try:\n",
    "    print('Accuracy:', round(metrics.accuracy_score(val_y, val_y_pred), 4))\n",
    "    print('Precision/PPV:', round(metrics.precision_score(val_y, val_y_pred), 4))\n",
    "    print('Recall', round(metrics.recall_score(val_y, val_y_pred), 4))\n",
    "    print('AUC:', round(metrics.roc_auc_score(val_y, val_y_pred), 4))\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect feature importance values\n",
    "\n",
    "try:\n",
    "    y = model.feature_importances_ # for Random Forest & XGBoost\n",
    "except AttributeError:\n",
    "    y = model.coef_[0] # for logistic regression\n",
    "\n",
    "d = {'q': X.columns, 'feat_imp': y} # create feature importance dataframe\n",
    "feat_imp = pd.DataFrame(data=d).sort_values(by='feat_imp', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge % missing with feature importances\n",
    "\n",
    "feat_imp = feat_imp.merge(q_nullseries.rename('% missing').to_frame(), left_index=False, left_on='q', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_decoder(question_code):\n",
    "    if '_split' in question_code:\n",
    "        question_code = question_code.replace('_split','')\n",
    "    question_code = question_code.upper()\n",
    "    \n",
    "    df = pd.read_excel('./data/Psychosocial data dictionary.xlsx', index_col=0)\n",
    "    questions = df['QUESTIONS']\n",
    "    try:\n",
    "        return questions[question_code]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "feat_imp['q_decoded'] = feat_imp['q'].apply(lambda x: question_decoder(x))\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"(%H:%M:%S)\")\n",
    "    current_date = now.date().strftime(\"%b-%d\")\n",
    "    \n",
    "    filename = current_date + current_time + \".html\"\n",
    "    print(\"saving results to \" + filename)\n",
    "    os.system(\"ipython nbconvert --to html mvp.ipynb\")\n",
    "    os.rename('mvp.html', \"./raw_results/\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
